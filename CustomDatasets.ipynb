{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Kira1108/huggingface-examples/blob/main/CustomDatasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you are working on ML projects, you are expected to write dirty code.     \n",
    "- Don't use design pattern at all.    \n",
    "- Don't use data structure algorithms at all.   \n",
    "- Use vectorized operations.    \n",
    "- Use encapsulated packages, like scikit-learn, tensorflow etc.      \n",
    "- Exploring data first and carefully.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Transformer Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eKPLPmeU52hv"
   },
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "\n",
    "# !pip install transformers datasets\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Raw Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJI6E9fA6IwF",
    "outputId": "524d675a-77fe-4c7d-eb39-67c9cdfde4b9"
   },
   "outputs": [],
   "source": [
    "# !wget -nc https://lazyprogrammer.me/course_files/AirlineTweets.csv\n",
    "# !mv AirlineTweets.csv /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5MRfg9Hj6hEd"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"artifacts\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean data for transformer training**    \n",
    "\n",
    "Althrough it is not a starndard way of transforming labels and prepare training dataset   \n",
    "It is required by transformers.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtifactStore:\n",
    "    \"\"\"`ArtifactStore` stores files that you created when doint ML.\n",
    "        :Param: artifacts_fq: root folder of artifacts path(default to `.artifacts`)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, artifacts_fp = \"./artifacts\"):\n",
    "        self.artifacts_fp = Path(artifacts_fp)\n",
    "        os.makedirs(self.artifacts_fp,exist_ok=True)\n",
    "\n",
    "    def log_binary(self, obj, fname):\n",
    "        fpath = self.artifacts_fp / f\"{fname}.joblib\"\n",
    "        dump(obj,fpath)\n",
    "        logger.info(f\"Dumped binary to {fpath}\")\n",
    "        \n",
    "    def load_binary(self, fname):\n",
    "        fpath = self.artifacts_fp / f\"{fname}.joblib\"\n",
    "        return load(fpath)\n",
    "        \n",
    "    def log_json(self, obj, fname):\n",
    "        fpath = self.artifacts_fp / f\"{fname}.json\"\n",
    "        json.dump(obj, open(fpath,'w'))\n",
    "        logger.info(f\"Dumped json file to {fpath}\")\n",
    "        \n",
    "    def load_json(self, fname):\n",
    "        fpath = self.artifacts_fp / f\"{fname}.json\"\n",
    "        return json.load(open(fpath,'r'))\n",
    "        \n",
    "    def log_label_encoder(self, label_encoder):\n",
    "        self.log_binary(label_encoder,\"label_encoder\")\n",
    "        \n",
    "        classmap = {i:c for i,c in enumerate(label_encoder.classes_)}\n",
    "        self.log_json(classmap,'label_encoder_classmap')\n",
    "            \n",
    "alog = ArtifactStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't try to write better code when doing ML.(Do that only if this code makes money for you)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label\n",
      "0                @VirginAmerica What @dhepburn said.      1\n",
      "1  @VirginAmerica plus you've added commercials t...      2\n",
      "2  @VirginAmerica I didn't today... Must mean I n...      1\n",
      "3  @VirginAmerica it's really aggressive to blast...      0\n",
      "4  @VirginAmerica and it's a really big bad thing...      0\n"
     ]
    }
   ],
   "source": [
    "# 0. do settings\n",
    "DATA_PATH = Path('./data')\n",
    "ARTIFACTS_PATH = Path(\"./artifacts\")\n",
    "\n",
    "# 1. read data\n",
    "df = pd.read_csv(DATA_PATH / \"AirlineTweets.csv\")[['text','airline_sentiment']]\n",
    "\n",
    "# 2. ml preprocessing\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['airline_sentiment'])\n",
    "\n",
    "# 3. do dirty column operations\n",
    "df.drop('airline_sentiment', axis = 1, inplace = True)\n",
    "df.rename(columns = {'text':'sentence'}, inplace = True)\n",
    "\n",
    "# 4. log whatever that will be used in the future\n",
    "alog.log_label_encoder(label_encoder)\n",
    "\n",
    "# 5. additional steps for your task\n",
    "df.to_csv(DATA_PATH / \"train_data.csv\", index = False)\n",
    "\n",
    "# 6. validate the steps above\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load artifacts for later use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alog.load_binary('label_encoder')\\\n",
    "    .transform(['negative','positive','neutral','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'negative', '1': 'neutral', '2': 'positive'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alog.load_json('label_encoder_classmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a csv dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fb7c561e8af0be40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/huan/.cache/huggingface/datasets/csv/default-fb7c561e8af0be40/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd76ad90bf0419c924ffc07da0f70ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70edf76b29b2497ab5f135f335f3db9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3b4437394f49a59499abb447b6ea16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/huan/.cache/huggingface/datasets/csv/default-fb7c561e8af0be40/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7408d9fff27a4c61879dd70b072f2ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 14640\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\", \n",
    "    data_files = str((DATA_PATH / \"train_data.csv\").resolve(strict = True))\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexed by position**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': '@VirginAmerica What @dhepburn said.', 'label': 1}\n",
      "{'sentence': \"@VirginAmerica plus you've added commercials to the experience... tacky.\", 'label': 2}\n",
      "{'sentence': \"@VirginAmerica I didn't today... Must mean I need to take another trip!\", 'label': 1}\n",
      "{'sentence': '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse', 'label': 0}\n",
      "{'sentence': \"@VirginAmerica and it's a really big bad thing about it\", 'label': 0}\n",
      "{'sentence': \"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\", 'label': 0}\n",
      "{'sentence': '@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)', 'label': 2}\n",
      "{'sentence': '@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP', 'label': 1}\n",
      "{'sentence': \"@virginamerica Well, I didn't…but NOW I DO! :-D\", 'label': 2}\n",
      "{'sentence': \"@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\", 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexed by column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica What @dhepburn said.',\n",
       " \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['sentence'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets are the same**     \n",
    "1. Dataset Object. A container for data, central object of a dataset framework.\n",
    "2. Dataset properties. Used to describe data.\n",
    "3. Dataset transformations. Alter a dataset and returns a new dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sort('label')['train'][:10]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 0, 0, 0, 2, 1, 2, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:10]['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**trian test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 10248\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 4392\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset = dataset['train'].train_test_split(test_size = 0.3)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aede377b099a4d73a2aa83c835c37cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a27db05267f4acbb24e1c64aeb1f5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['sentence'], truncation = True)\n",
    "\n",
    "tokenized_dataset = split_dataset.map(tokenize_fn, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 23:10:50.548624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = ARTIFACTS_PATH / \"training_dir\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis = -1)\n",
    "    return {\"accuracy\": np.mean(predictions == labels),\"f1\":f1_score(y_true = labels, y_pred = predictions, average = 'weighted')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = training_args, \n",
    "    train_dataset = tokenized_dataset['train'], \n",
    "    eval_dataset = tokenized_dataset['test'],\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMoQ1X/dF6LkRBcnDFfmHe9",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
