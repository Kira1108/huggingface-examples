{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Kira1108/huggingface-examples/blob/main/CustomDatasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you are working on ML projects, you are expected to write dirty code.     \n",
    "- Don't use design pattern at all.    \n",
    "- Don't use data structure algorithms at all.   \n",
    "- Use vectorized operations.    \n",
    "- Use encapsulated packages, like scikit-learn, tensorflow etc.      \n",
    "- Exploring data first and carefully.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Transformer Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eKPLPmeU52hv"
   },
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "\n",
    "# !pip install transformers datasets\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Raw Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJI6E9fA6IwF",
    "outputId": "524d675a-77fe-4c7d-eb39-67c9cdfde4b9"
   },
   "outputs": [],
   "source": [
    "# !wget -nc https://lazyprogrammer.me/course_files/AirlineTweets.csv\n",
    "# !mv AirlineTweets.csv /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5MRfg9Hj6hEd"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level = logging.DEBUG)\n",
    "logger = logging.getLogger(\"artifacts\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean data for transformer training**    \n",
    "\n",
    "Althrough it is not a starndard way of transforming labels and prepare training dataset   \n",
    "It is required by transformers.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtifactStore:\n",
    "    \"\"\"`ArtifactStore` stores files that you created when doint ML.\n",
    "        :Param: artifacts_fq: root folder of artifacts path(default to `.artifacts`)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, artifacts_fp = \"./artifacts\"):\n",
    "        self.artifacts_fp = Path(artifacts_fp)\n",
    "        os.makedirs(self.artifacts_fp,exist_ok=True)\n",
    "\n",
    "    def log_binary(self, obj, fname):\n",
    "        fpath = self.artifacts_fp / f\"{fname}.joblib\"\n",
    "        dump(obj,fpath)\n",
    "        logger.info(f\"Dumped binary to {fpath}\")\n",
    "        \n",
    "    def load_binary(self, fname):\n",
    "        fpath = self.artifacts_fp / f\"{fname}.joblib\"\n",
    "        return load(fpath)\n",
    "        \n",
    "    def log_json(self, obj, fname):\n",
    "        fpath = self.artifacts_fp / f\"{fname}.json\"\n",
    "        json.dump(obj, open(fpath,'w'))\n",
    "        logger.info(f\"Dumped json file to {fpath}\")\n",
    "        \n",
    "    def load_json(self, fname):\n",
    "        fpath = self.artifacts_fp / f\"{fname}.json\"\n",
    "        return json.load(open(fpath,'r'))\n",
    "        \n",
    "    def log_label_encoder(self, label_encoder):\n",
    "        self.log_binary(label_encoder,\"label_encoder\")\n",
    "        \n",
    "        classmap = {i:c for i,c in enumerate(label_encoder.classes_)}\n",
    "        self.log_json(classmap,'label_encoder_classmap')\n",
    "            \n",
    "alog = ArtifactStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't try to write better code when doing ML.(Do that only if this code makes money for you)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:artifacts:Dumped binary to artifacts/label_encoder.joblib\n",
      "INFO:artifacts:Dumped json file to artifacts/label_encoder_classmap.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label\n",
      "0                @VirginAmerica What @dhepburn said.      1\n",
      "1  @VirginAmerica plus you've added commercials t...      2\n",
      "2  @VirginAmerica I didn't today... Must mean I n...      1\n",
      "3  @VirginAmerica it's really aggressive to blast...      0\n",
      "4  @VirginAmerica and it's a really big bad thing...      0\n"
     ]
    }
   ],
   "source": [
    "# 0. do settings\n",
    "DATA_PATH = Path('./data')\n",
    "ARTIFACTS_PATH = Path(\"./artifacts\")\n",
    "\n",
    "# 1. read data\n",
    "df = pd.read_csv(DATA_PATH / \"AirlineTweets.csv\")[['text','airline_sentiment']]\n",
    "\n",
    "# 2. ml preprocessing\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['airline_sentiment'])\n",
    "\n",
    "# 3. do dirty column operations\n",
    "df.drop('airline_sentiment', axis = 1, inplace = True)\n",
    "df.rename(columns = {'text':'sentence'}, inplace = True)\n",
    "\n",
    "# 4. log whatever that will be used in the future\n",
    "alog.log_label_encoder(label_encoder)\n",
    "\n",
    "# 5. additional steps for your task\n",
    "df.to_csv(DATA_PATH / \"train_data.csv\", index = False)\n",
    "\n",
    "# 6. validate the steps above\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load artifacts for later use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alog.load_binary('label_encoder')\\\n",
    "    .transform(['negative','positive','neutral','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'negative', '1': 'neutral', '2': 'positive'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alog.load_json('label_encoder_classmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a csv dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/csv/csv.py HTTP/1.1\" 200 0\n",
      "WARNING:datasets.builder:Using custom data configuration default-efc5761841ea5a32\n",
      "WARNING:datasets.builder:Found cached dataset csv (/home/wanghuan/.cache/huggingface/datasets/csv/default-efc5761841ea5a32/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbdfd77d61141c7bb95b2810ab3ded4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 14640\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\", \n",
    "    data_files = str((DATA_PATH / \"train_data.csv\").resolve(strict = True))\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexed by position**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': '@VirginAmerica What @dhepburn said.', 'label': 1}\n",
      "{'sentence': \"@VirginAmerica plus you've added commercials to the experience... tacky.\", 'label': 2}\n",
      "{'sentence': \"@VirginAmerica I didn't today... Must mean I need to take another trip!\", 'label': 1}\n",
      "{'sentence': '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse', 'label': 0}\n",
      "{'sentence': \"@VirginAmerica and it's a really big bad thing about it\", 'label': 0}\n",
      "{'sentence': \"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\", 'label': 0}\n",
      "{'sentence': '@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)', 'label': 2}\n",
      "{'sentence': '@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP', 'label': 1}\n",
      "{'sentence': \"@virginamerica Well, I didn't…but NOW I DO! :-D\", 'label': 2}\n",
      "{'sentence': \"@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\", 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexed by column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica What @dhepburn said.',\n",
       " \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\"]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['sentence'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets are the same**     \n",
    "1. Dataset Object. A container for data, central object of a dataset framework.\n",
    "2. Dataset properties. Used to describe data.\n",
    "3. Dataset transformations. Alter a dataset and returns a new dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /home/wanghuan/.cache/huggingface/datasets/csv/default-efc5761841ea5a32/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d4524d6f2a9b2c90.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sort('label')['train'][:10]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 0, 0, 0, 2, 1, 2, 2]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:10]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMoQ1X/dF6LkRBcnDFfmHe9",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
